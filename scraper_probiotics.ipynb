{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca35b901",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Scanning https://tabletsindia.com...\n",
      "{\n",
      "    \"A_Identity\": {\n",
      "        \"Company_Name\": \"Tablets India\",\n",
      "        \"Website\": \"https://tabletsindia.com\",\n",
      "        \"Tagline\": \"Tablets India offers advanced probiotics, nutraceuticals and pharma solutions designed to support immunity, gut health and overall wellness for all age groups.\"\n",
      "    },\n",
      "    \"B_Business_Summary\": {\n",
      "        \"What_They_Do\": \"Tablets (India) Limited was one of the early pharma ventures in India and was founded in 1938 by Mr Sri Krishna Jhaver.\",\n",
      "        \"Primary_Offerings\": [\n",
      "            \"Capsule in capsule\",\n",
      "            \"Sachet in Sachet\",\n",
      "            \"Oral Dispersible Powder\"\n",
      "        ],\n",
      "        \"Target_Segments\": []\n",
      "    },\n",
      "    \"C_Evidence\": {\n",
      "        \"Key_Pages_Detected\": [\n",
      "            \"Science: https://tabletsindia.com/clinical-research\",\n",
      "            \"Contact: https://tabletsindia.com/contact-us\",\n",
      "            \"About: https://tabletsindia.com/about-us\",\n",
      "            \"Careers: https://tabletsindia.com/careers\",\n",
      "            \"Contact: https://tabletsindia.com/human-health/fertility-support/fertihope-m\",\n",
      "            \"Science: https://tabletsindia.com/rd-activities\"\n",
      "        ],\n",
      "        \"Signals_Found\": [\n",
      "            \"Immunity\",\n",
      "            \"Patent\",\n",
      "            \"Partners\",\n",
      "            \"Research\",\n",
      "            \"Clinical Trial\",\n",
      "            \"Copyright\",\n",
      "            \"Science\",\n",
      "            \"All Rights Reserved\"\n",
      "        ],\n",
      "        \"Social_Links\": [\n",
      "            \"https://www.facebook.com/TabletsIndia\",\n",
      "            \"https://www.youtube.com/channel/UCQ2BbybacrxLP_ugHycIb-w/videos\",\n",
      "            \"https://www.instagram.com/tabletsindia/\",\n",
      "            \"https://twitter.com/TabletsIndiaLtd\",\n",
      "            \"https://www.linkedin.com/company/tabletsindia/\"\n",
      "        ]\n",
      "    },\n",
      "    \"D_Contact_Location\": {\n",
      "        \"Emails\": [\n",
      "            \"info@tabletsindia.com\"\n",
      "        ],\n",
      "        \"Phones\": [],\n",
      "        \"Address\": \"Jhaver Centre 72 Marshalls Road, Chennai \\u2013 600 008 600 008\",\n",
      "        \"Contact_URL\": \"https://tabletsindia.com/contact-us\"\n",
      "    },\n",
      "    \"E_Team_Hiring\": {\n",
      "        \"Careers_URL\": \"https://tabletsindia.com/careers\",\n",
      "        \"Roles_Mentioned\": []\n",
      "    },\n",
      "    \"F_Metadata\": {\n",
      "        \"Timestamp\": \"2025-12-23 01:34:06\",\n",
      "        \"Pages_Visited\": [\n",
      "            \"https://tabletsindia.com\",\n",
      "            \"https://tabletsindia.com/about-us\",\n",
      "            \"https://tabletsindia.com/clinical-research\",\n",
      "            \"https://tabletsindia.com/contact-us\",\n",
      "            \"https://tabletsindia.com/human-health/fertility-support/fertihope-m\",\n",
      "            \"https://tabletsindia.com/rd-activities\"\n",
      "        ],\n",
      "        \"Errors\": []\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION: Signals Database ---\n",
    "SIGNALS_DB = {\n",
    "    \"Generic\": [\n",
    "        \"Case Study\", \"Testimonials\", \"Awards\", \"Certifications\", \"Clients\", \n",
    "        \"Partners\", \"ISO 9001\", \"Patent\", \"Copyright\", \"All Rights Reserved\"\n",
    "    ],\n",
    "    \"Probiotics_Specific\": [\n",
    "        \"CFU\", \"Strain\", \"Lactobacillus\", \"Bifidobacterium\", \"L. casei\", \"Shirota\",\n",
    "        \"Spore-forming\", \"Lyophilization\", \"Microencapsulation\", \"Fermentation\", \n",
    "        \"Gut Health\", \"Immunity\", \"Clinical Trial\", \"Science\", \"Research\", \n",
    "        \"GRAS\", \"FSSAI\", \"GMP Certified\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "class UniversalScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url if base_url.startswith(\"http\") else \"https://\" + base_url\n",
    "        self.domain = urlparse(self.base_url).netloc\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        \n",
    "        # Output Structure\n",
    "        self.data = {\n",
    "            \"A_Identity\": {\"Company_Name\": None, \"Website\": self.base_url, \"Tagline\": None},\n",
    "            \"B_Business_Summary\": {\"What_They_Do\": \"\", \"Primary_Offerings\": [], \"Target_Segments\": []},\n",
    "            \"C_Evidence\": {\"Key_Pages_Detected\": [], \"Signals_Found\": [], \"Social_Links\": []},\n",
    "            \"D_Contact_Location\": {\"Emails\": [], \"Phones\": [], \"Address\": None, \"Contact_URL\": None},\n",
    "            \"E_Team_Hiring\": {\"Careers_URL\": None, \"Roles_Mentioned\": []},\n",
    "            \"F_Metadata\": {\"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Pages_Visited\": [], \"Errors\": []}\n",
    "        }\n",
    "        self.visited = set()\n",
    "        self.max_pages = 12\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        \"\"\"Fetches URL with basic limits.\"\"\"\n",
    "        if url in self.visited or len(self.visited) >= self.max_pages: return None\n",
    "        self.visited.add(url)\n",
    "        self.data[\"F_Metadata\"][\"Pages_Visited\"].append(url)\n",
    "        try:\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            if resp.status_code == 200: return BeautifulSoup(resp.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            self.data[\"F_Metadata\"][\"Errors\"].append(f\"Failed {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    def get_text_safe(self, soup):\n",
    "        \"\"\"Prevents text merging by adding space separators.\"\"\"\n",
    "        if not soup: return \"\"\n",
    "        return re.sub(r'\\s+', ' ', soup.get_text(separator=' ')).strip()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # --- CORE LOGIC 1: Contact & Address Extraction ---\n",
    "    def extract_contacts(self, soup):\n",
    "        \"\"\"Accepts SOUP object to handle footer text separation correctly.\"\"\"\n",
    "        text = self.get_text_safe(soup)\n",
    "        \n",
    "        # 1. PHONE EXTRACTION (Strict Logic)\n",
    "        phone_patterns = [\n",
    "            r'(?:Ph|Phone|Tel|Fax|Call|Mobile)[:\\-\\.\\s]{0,3}([\\+\\(]?\\d{1,4}[\\)\\s\\.-]{0,3}\\d{2,5}[\\s\\.-]?\\d{3,4}[\\s\\.-]?\\d{3,5})', # Labeled\n",
    "            r'(?:\\+91|011|022|044)[\\s\\-]??\\d{3,5}[\\s\\-]??\\d{3,5}', # Indian Landlines\n",
    "            r'1800[\\s-]?\\d{3}[\\s-]?\\d{3,4}' # Toll Free\n",
    "        ]\n",
    "        for pat in phone_patterns:\n",
    "            matches = re.findall(pat, text, re.IGNORECASE)\n",
    "            for m in matches:\n",
    "                clean_num = re.sub(r'[^\\d+]', '', m)\n",
    "                if 8 <= len(clean_num) <= 14:\n",
    "                    self.data[\"D_Contact_Location\"][\"Phones\"].append(m.strip())\n",
    "\n",
    "        # 2. EMAIL EXTRACTION\n",
    "        self.data[\"D_Contact_Location\"][\"Emails\"].extend(\n",
    "            re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', text)\n",
    "        )\n",
    "\n",
    "        # 3. ADDRESS EXTRACTION (Robust Context + Pincode Logic)\n",
    "        if not self.data[\"D_Contact_Location\"][\"Address\"]:\n",
    "            mandatory_context = [\n",
    "                \"New Delhi\", \"Mumbai\", \"Bangalore\", \"Bengaluru\", \"Chennai\", \"Kolkata\", \n",
    "                \"Hyderabad\", \"Pune\", \"Gurgaon\", \"Noida\", \"Baddi\", \"Vadodara\", \"Nashik\", \n",
    "                \"India\", \"State\", \"City\", \"District\", \"Tamil Nadu\", \"Delhi\", \"Egmore\"\n",
    "            ]\n",
    "            markers = [\n",
    "                \"Plot No\", \"Shop No\", \"Unit\", \"Building\", \"Tower\", \"Floor\", \n",
    "                \"Sector\", \"Phase\", \"Block\", \"Industrial Area\", \"Industrial Estate\", \n",
    "                \"MIDC\", \"GIDC\", \"Street\", \"Road\", \"Rd\", \"Lane\", \"Opp\", \"Near\", \n",
    "                \"Jhaver\", \"Complex\" # Added specific markers for Tablets India\n",
    "            ]\n",
    "\n",
    "            # Regex matches 6-digit Pincodes, allowing optional space (e.g., 600 008)\n",
    "            zip_matches = re.finditer(r'\\b\\d{3}\\s?\\d{3}\\b', text)\n",
    "            \n",
    "            for z in zip_matches:\n",
    "                end_pos = z.end()\n",
    "                start_pos = max(0, end_pos - 200)\n",
    "                candidate = text[start_pos:end_pos]\n",
    "                \n",
    "                # Check 1: Must contain City/State\n",
    "                if any(c.lower() in candidate.lower() for c in mandatory_context):\n",
    "                    # Check 2: Must contain an Address Marker\n",
    "                    if any(m.lower() in candidate.lower() for m in markers):\n",
    "                        \n",
    "                        # Find the start of the address\n",
    "                        pattern = r'(?:' + '|'.join(re.escape(m) for m in markers) + r').+'\n",
    "                        match = re.search(pattern, candidate, re.IGNORECASE)\n",
    "                        if match:\n",
    "                            final_addr = match.group(0).strip()\n",
    "                            if \"et al\" not in final_addr and len(final_addr) > 15:\n",
    "                                self.data[\"D_Contact_Location\"][\"Address\"] = final_addr + \" \" + z.group(0)\n",
    "                                break\n",
    "\n",
    "    # --- CORE LOGIC 2: Hybrid Offering Extractor ---\n",
    "    def extract_offerings_hybrid(self, soup):\n",
    "        \"\"\"Tiered extraction: Links (Catalogs) -> Headers (Services) -> Images (Visual Sites).\"\"\"\n",
    "        offerings = []\n",
    "        blacklist = [\n",
    "            \"learn more\", \"read more\", \"view all\", \"add to cart\", \"buy now\", \n",
    "            \"quick view\", \"subscribe\", \"login\", \"search\", \"menu\", \"account\",\n",
    "            \"recently viewed\", \"related products\", \"description\", \"ingredients\"\n",
    "        ]\n",
    "        \n",
    "        # TIER 1: Link-Based (Catalog Sites)\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].lower()\n",
    "            text = self.clean_text(a.text)\n",
    "            if 4 < len(text) < 50:\n",
    "                if any(k in href for k in ['/product', '/item', '/shop', '/store']):\n",
    "                    if not any(b in text.lower() for b in blacklist):\n",
    "                        offerings.append(text)\n",
    "\n",
    "        # TIER 2: Header-Based (Service/Brand Sites)\n",
    "        for tag in soup.find_all(['h3', 'h4', 'h5']):\n",
    "            if tag.find_parent(['nav', 'footer', 'header']): continue\n",
    "            text = self.clean_text(tag.text)\n",
    "            if 5 < len(text) < 45 and not any(b in text.lower() for b in blacklist):\n",
    "                if text[0].isupper(): offerings.append(text)\n",
    "\n",
    "        # TIER 3: Visual/Alt-Text (Fallback)\n",
    "        if len(offerings) < 3:\n",
    "            for img in soup.find_all('img', alt=True):\n",
    "                alt = self.clean_text(img['alt'])\n",
    "                if any(k in alt.lower() for k in ['bottle', 'pack', 'capsule', 'sachet', 'powder']):\n",
    "                    if 5 < len(alt) < 40: offerings.append(alt)\n",
    "\n",
    "        return list(set(offerings))[:15]\n",
    "\n",
    "    def extract_signals(self, text):\n",
    "        for cat, keywords in SIGNALS_DB.items():\n",
    "            found = [k for k in keywords if k.lower() in text.lower()]\n",
    "            self.data[\"C_Evidence\"][\"Signals_Found\"].extend(found)\n",
    "\n",
    "    # --- EXECUTION LOOP ---\n",
    "    def run(self):\n",
    "        print(f\"ðŸš€ Scanning {self.base_url}...\")\n",
    "        soup_home = self.get_soup(self.base_url)\n",
    "        if not soup_home: return {\"Error\": \"Site Unreachable\"}\n",
    "\n",
    "        # 1. Identity\n",
    "        self.data[\"A_Identity\"][\"Company_Name\"] = soup_home.title.string.split('|')[0].strip() if soup_home.title else self.domain\n",
    "        meta = soup_home.find('meta', attrs={'name': 'description'})\n",
    "        if meta: self.data[\"A_Identity\"][\"Tagline\"] = meta['content'].strip()\n",
    "\n",
    "        # 2. Initial Scan (Home) - PASSING SOUP for text fusion fix\n",
    "        self.extract_contacts(soup_home)\n",
    "        self.extract_signals(self.get_text_safe(soup_home))\n",
    "\n",
    "        # 3. Intelligent Navigation Queue\n",
    "        priority_queue = []\n",
    "        nav_map = {\n",
    "            \"contact\": \"Contact\", \"support\": \"Contact\",\n",
    "            \"about\": \"About\", \"story\": \"About\", \"heritage\": \"About\",\n",
    "            \"career\": \"Careers\", \"job\": \"Careers\",\n",
    "            \"product\": \"Products\", \"shop\": \"Products\", \"flavour\": \"Products\",\n",
    "            \"science\": \"Science\", \"research\": \"Science\", \"technology\": \"Science\"\n",
    "        }\n",
    "\n",
    "        for a in soup_home.find_all('a', href=True):\n",
    "            href = urljoin(self.base_url, a['href'])\n",
    "            text = a.text.lower()\n",
    "            \n",
    "            # Socials\n",
    "            if any(x in href for x in ['linkedin', 'twitter', 'facebook', 'instagram', 'youtube']):\n",
    "                self.data[\"C_Evidence\"][\"Social_Links\"].append(href)\n",
    "                continue\n",
    "\n",
    "            # Internal Pages\n",
    "            if self.domain in href:\n",
    "                for kw, category in nav_map.items():\n",
    "                    if kw in text or kw in href:\n",
    "                        entry = f\"{category}: {href}\"\n",
    "                        if entry not in self.data[\"C_Evidence\"][\"Key_Pages_Detected\"]:\n",
    "                            self.data[\"C_Evidence\"][\"Key_Pages_Detected\"].append(entry)\n",
    "                        \n",
    "                        if category in [\"Contact\", \"Products\", \"Science\", \"About\"]:\n",
    "                            priority_queue.append((href, category))\n",
    "                        if category == \"Contact\": self.data[\"D_Contact_Location\"][\"Contact_URL\"] = href\n",
    "                        if category == \"Careers\": self.data[\"E_Team_Hiring\"][\"Careers_URL\"] = href\n",
    "                        break\n",
    "\n",
    "        # 4. Deep Crawl\n",
    "        for url, cat in priority_queue[:8]:\n",
    "            sub_soup = self.get_soup(url)\n",
    "            if not sub_soup: continue\n",
    "            \n",
    "            # Extract Contacts on sub-pages (Critical for Contact pages)\n",
    "            self.extract_contacts(sub_soup)\n",
    "            self.extract_signals(self.get_text_safe(sub_soup))\n",
    "\n",
    "            if cat in [\"Products\", \"Science\"]:\n",
    "                items = self.extract_offerings_hybrid(sub_soup)\n",
    "                self.data[\"B_Business_Summary\"][\"Primary_Offerings\"].extend(items)\n",
    "            \n",
    "            if cat == \"About\" and not self.data[\"B_Business_Summary\"][\"What_They_Do\"]:\n",
    "                paras = [p.text for p in sub_soup.find_all('p') if len(p.text) > 80]\n",
    "                if paras: self.data[\"B_Business_Summary\"][\"What_They_Do\"] = self.clean_text(paras[0])\n",
    "\n",
    "        # 5. Final Deduplication\n",
    "        for section in [\"C_Evidence\", \"D_Contact_Location\", \"B_Business_Summary\"]:\n",
    "            for key, val in self.data[section].items():\n",
    "                if isinstance(val, list):\n",
    "                    self.data[section][key] = list(set(val))\n",
    "        \n",
    "        return self.data\n",
    "\n",
    "# --- RUN EXAMPLE ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Change URL to test different sites\n",
    "    # url = \"https://velbiom.com\"\n",
    "    # url = \"https://yakult.co.in\"\n",
    "    url = \"https://tabletsindia.com\"\n",
    "    \n",
    "    scraper = UniversalScraper(url) \n",
    "    print(json.dumps(scraper.run(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cfd04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
